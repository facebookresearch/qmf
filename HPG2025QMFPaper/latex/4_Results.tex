\section{Results}\label{sec:results}
Our results are captured on a Jetson Orin Nano 8GB, which provides a 6-core Arm Cortex-A78AE, a 1024 CUDA-core GPU, and 8 GB of  128-bit LPDDR5 RAM.   We run the Jetson in \SI{15}{\watt} mode. For our CPU results, we use only a single CPU core for runtime animation.  We demonstrate our technique with 6 blendshape datasets.  \Table{tab:FullResults} and \Figure{fig:Datasets} show our datasets.  Aura, Bowen, Jupiter, and Proteus are publicly available models \cite{kavan2024compressed}.  Louise is the Animatable Digital Double of Louise by EiskoÂ© ( \href{http://www.eisko.com}{http://www.eisko.com} ).  We have also created a new face model, Carlos.
In our experiments, we use a fairly traditional sparse blendshapes format as a baseline.  $\matA$ is stored as an array of blendshape where each blendshape is an array of vertex indices and a corresponding array of non-zero deltas.  As part of our preprocessing pipeline, deltas smaller than $10 \mu m$ in absolute value are clamped to zero. 
The vertex indices for the sparse blendshape representation are 32-bit integers, and each non-zero delta consists of three 32-bit floating point values.

We compare our method against \csfb. For each dataset we use 400 bones, 8 weights per vertex, and 6,000 non-zero values for transformations, except the Louise and Carlos datasets, where 20,000 non-zero values were required for high-quality results. 
The chosen number of bones is higher than in \cite{kavan2024compressed} to allow \csfb better capture small details, while maintaining the same number of non-zeros for transformation matrices. This ensures a fair quality comparison with our matrix factorization methods.
We first ran experiments to see the effect of applying our new loss function (\Eq{eq:NewLoss}) on \csfb.  We observed significant quality improvements in wrinkle areas  (see \Figure{fig:CSFBAura}) and slight improvements in metrics (see \Table{tab:FullResults}).  Having established that our new loss function produces better results, the remainder of comparisons against our sparse matrix factorization technique will be using \csfb with the new loss function.

\begin{figure}
    \includegraphics[width=\linewidth]{chooseAlpha.pdf}
    \caption{Choosing $\alpha$ (Bowen model).}
    \label{fig:chooseAlpha}
\end{figure}

To demonstrate the synergistic effect of the new loss function with the wrinkles map, we created a synthetic dataset by modifying the Jupiter dataset so that every second blendshape is a copy of the blendshape "right eyebrows up".   In \Figure{fig:QMFNewNormAndWrinkles}, we can see (a) that with the original loss function (\Eq{eq:OldLoss}), we are never able to recover wrinkles.  Additionally, we can see (b) that by applying the new loss function (\Eq{eq:NewLoss}), we are able to recover wrinkles on the right side of the face, while we continue to lose them on the left side of the face.  (c) shows that by applying both the new loss function and wrinkles map, we reliably recover both sets of wrinkles. While not shown explicitly in the figure, using the wrinkle map with the old loss function leads to no discernible change from (a).

To measure the quality of optimization and to compare algorithms we use three metrics.  The first two are
mean absolute error (MAE) and maximum absolute error (MXE), corresponding to $L^1$ and $L^\infty$ norms.
MAE is the mean over all vertices of $|\matA - \matB\matC|$ and $\mathrm{MXE} = \max{|\matA - \matB\matC|}$ over all vertices.  The third metric attempts to measure noise introduced by optimization and quantization, and we use the average edge angular difference (EAD). This is calculated as the mean of $ | \beta_{A} - \beta_{BC} | $  over all edges, where $\beta$ represents the dihedral angle between an edge's adjacent faces.

To determine the optimal value of parameter $\alpha$, we conducted multiple optimization runs for all models and plotted the resulting curves for our metrics. Notably, all models exhibited a consistent behavior: as
$\alpha$ increased, the noise metric improved rapidly, while the position metrics degraded almost linearly (see \Figure{fig:chooseAlpha}) For every model we choose the biggest $\alpha$ value that did not significantly degrade the position metrics.

%----
To compare \csfb with sparse matrix factorization, we selected parameters that achieved the same or slightly better quality of optimization, allowing us to compare sizes and runtime costs. 
The number of non-zero values was chosen within a range of 35,000 to 140,000, depending on the level of detail in the model and the complexity of the blendshapes.
The value of $\wrinklesMult$ was selected from a range of 0.0006 to 0.003.
As shown in \Figure{fig:Teaser} and  \Figure{fig:QMFNewNormAndWrinkles}, the sparse factorization method preserves details in challenging wrinkle areas. 
%
 The resulting compression rates are illustrated in \Figure{fig:Compression}.  \qmf demonstrates a 3.2$\times$ to 4.6$\times$ higher compression rate than \csfb.

To assess the differences between algorithms in a real-time environment, we implemented sparse blendshapes, \csfb, and sparse matrix factorization in C++ and in CUDA. The \csfb implementation was highly optimized and works directly with a sparse matrix of bone transformations with only 6 degrees of freedom for each bone.  Like \qmf, sparse blendshapes required some auxiliary precomputed data for efficient GPU computation. \csfb does not require extra information for efficient processing on the GPU.

We executed the same animation sequence with precomputed blendshape weights with each algorithm and measured average wall clock time, with results shown in \Figure{fig:RuntimeCost}. \qmf selects 8-bit quantization for the Aura and Jupiter models, and 10.66-bit as the smallest quantization and 16-bit as the fastest runtime quantization for all other models. 
We can observe that our sparse factorization methods are significantly faster than an optimized version of \csfb. \qmf can be slightly slower than non-quantized sparse matrix factorization due to additional costs for dequantization and because the non-quantized sparse matrix factorization data already fits in CPU cache.  

When evaluating on the GPU, we noted that a common scenario would be to animate multiple faces per frame. To that end, we run 10 animations concurrently.  Results for our CUDA implementation are shown in \Figure{fig:RuntimeCostGPU}.  We note that
10.66-bit quantization incurs a fairly heavy runtime penalty of about 30\% on the CPU, but a smaller penalty up to about 15\% on the GPU.

Detailed results for all experiments are shown in \Table{tab:FullResults}.   Full optimization code for our method will be released open source concurrent to the conference. 


\begin{figure}
    \includegraphics[width=\linewidth]{durationDiagramCPU.pdf}
    \caption{CPU: Average facial expression calculation wall time}
    \label{fig:RuntimeCost}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{durationDiagramCUDA.pdf}
    \caption{GPU: Average wall time for 10 concurrent facial expression calculations}
    \label{fig:RuntimeCostGPU}
\end{figure}


\input{generated/bigTable.tex}

%--
%\input{generated/modelsTable.tex}

\begin{figure}
    \includegraphics[width=0.31\linewidth]{aura_forehead_orig.png}
    \includegraphics[width=0.31\linewidth]{aura_forehead_SD0.png}
    \includegraphics[width=0.31\linewidth]{aura_forehead_SD1.png} \\
    \vspace{-1em}
    \includegraphics[width=0.31\linewidth, trim={0cm 39cm 17cm 12cm},clip]{aura_forehead_orig.png}
    \includegraphics[width=0.31\linewidth, trim={0cm 39cm 17cm 12cm},clip]{aura_forehead_SD0.png}
    \includegraphics[width=0.31\linewidth, trim={0cm 39cm 17cm 12cm},clip]{aura_forehead_SD1.png}
    \vspace{1em}
    \caption{\csfb results left to right: sparse blendshapes, old loss, new loss }
    \label{fig:CSFBAura}
\end{figure}


\begin{figure}
    \includegraphics[width=\linewidth]{compressionRatesDiagram.pdf}
    \caption{Compression Rates relative to sparse blendshapes size}
    \label{fig:Compression}
\end{figure}


%\begin{figure}
%    \includegraphics[width=\linewidth]{example-image-a}
%    \caption{Errors}
%    \label{Fig:Errors}
%\end{figure}

%%\input{generated/bigTable.tex}
% Epic Games meta Human whitepaper https://cdn2.unrealengine.com/rig-logic-whitepaper-v2-5c9f23f7e210.pdf
