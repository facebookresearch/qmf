\documentclass[acmtog,anonymous,review]{acmart}
\acmSubmissionID{papers\_1996s1}

\usepackage{booktabs} % For formal tables
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{siunitx}
% \usepackage{float}

\graphicspath{ {./images/} {./generated/} }

\lstset{language=C++,
                basicstyle=\ttfamily\scriptsize,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment=[l][\color{magenta}]{\#}
}
\newsavebox{\codebox}
% TOG prefers author-name bib system with square brackets
\citestyle{acmauthoryear}
%\setcitestyle{nosort,square} % nosort to allow for manual chronological ordering



\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\setlength{\algomargin}{0pt}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
%\SetAlCapNameFnt{\small}https://overleaf.thefacebook.com/project/6778e7795bc9a40e2bfdb80e
\SetAlCapHSkip{0pt}

% Metadata Information
%\acmJournal{TOG}https://overleaf.thefacebook.com/project/6778e7795bc9a40e2bfdb80e


\input{Commands.tex}

% Document starts
\begin{document}
\setlength{\abovecaptionskip}{0.5ex}
\setlength{\belowcaptionskip}{0.5ex}
\setlength{\floatsep}{0.5ex}
\setlength{\textfloatsep}{0.5ex}

\title{Supplemental Material for QMF-Blend: Quantized Matrix Factorization for Efficient Blendshape Compression}

\maketitle
% \section{Supplemental Material}
% \vspace{4mm}
\subsection*{CPU Runtime Code}
This supplementary material provides the CPU implementation for applying blendshapes at runtime using sparse matrix factorization.

To perform the multiplication efficiently, we iterate over the array~$\bSWeight$. For each non-zero weight, we access the corresponding three rows of the matrix $\matB$. If $\matB$ contains non-zero entries for the given weight, we perform the necessary multiplications and accumulate the results into the intermediate matrix $\matWB$.

In the final stage of the algorithm, we multiply the small dense matrix $\matWB$ with sparse matrix $\matC$. This is done by iterating over the columns of $\matC$. For each non-zero entry, we access the corresponding column in $\matWB$, perform the multiplication, and add the result to the precomputed rest pose value. The final result is then stored. (See \Algorithm{alg:RuntimeCode})
% \vspace{5cm}

% \subsection*{Chose of $\alpha$ value}
% \begin{figure}[h]
%     \includegraphics[width=\linewidth]{chooseAlpha.pdf}
%     \caption{Choosing $\alpha$ (Bowen model).}
%     \label{fig:chooseAlpha}
% \end{figure}
% \Figure{fig:chooseAlpha} illustrates how the evaluation metrics vary with the parameter $\alpha$. As $\alpha$ increases, the noise metric improves rapidly, while the position metric gradually degrades in an almost linear fashion. We observed this behavior consistently across all models in our dataset. For each model, we selected the largest $\alpha$ value that did not cause a significant degradation in the position metric.

\begin{lrbox}{\codebox}
\begin{lstlisting}[language=c++]
template <typename BlendShapeData, typename ValueType>
void computeSparseFactorizationBlendshapes(
    const BlendShapeData& data,
    DeviceSpan<const Vec3>& staticMeshCoords,
    DeviceSpan<Vec3>& outMeshCoords) {
  constexpr bool isQuantized = !std::is_floating_point_v<ValueType>;
  float bMult = data.bRange / std::numeric_limits<ValueType>::max();
  float cMult = data.cRange / std::numeric_limits<ValueType>::max();

  std::vector<float> matrixWB(data.numColumnB * 3lu, 0.f);
  uint32_t flatBIndex = 0;
  for (uint32_t bsIndex = 0; bsIndex < data.numBS; ++bsIndex) {
    float bsWeight = data.blendshapeWeights[bsIndex];
    for (uint32_t i = 0; i < 3; ++i) {
      uint32_t rowIndex = bsIndex * 3 + i;
      uint32_t numValuesRow = data.matrixBNumValuesRow[rowIndex];
      for (uint32_t j = 0; j < numValuesRow; ++j) {
        float bValue = isQuantized ?
          data.bMin + bMult * data.matrixB[flatBIndex] : 
          data.matrixB[flatBIndex];
        uint32_t bCol = data.matrixBIndices[flatBIndex];
        matrixWB[3 * bCol + i] += bValue * bsWeight;
        flatBIndex++;
      }
    }
  }
 
  size_t numVertices = outMeshCoords.size();
  uint32_t flatCIndex = 0;
  for (uint32_t vIndex = 0; vIndex < numVertices; ++vIndex) {
    Vec3 res(0.f);
    uint_fast16_t numValuesCol = data.matrixCNumValuesCol[vIndex];
    for (uint_fast16_t i = 0; i < numValuesCol; ++i) {
      uint32_t cRow = data.matrixCIndices[flatCIndex + i];
      float cValue = isQuantized ?
        data.cMin + cMult * data.matrixC[flatCIndex + i] : 
        data.matrixC[flatCIndex + i];
      res.x += matrixWB[3 * cRow + 0] * cValue;
      res.y += matrixWB[3 * cRow + 1] * cValue;
      res.z += matrixWB[3 * cRow + 2] * cValue;
    }
    flatCIndex += data.matrixCNumValuesCol[vIndex];
    outMeshCoords[vIndex] = staticMeshCoords[vIndex] + res;
  }
}
\end{lstlisting}
\end{lrbox}

\begin{algorithm}[b]
\caption{CPU code to apply sparse matrix factorization-based blendshapes at runtime.}
\label{alg:RuntimeCode}
\usebox{\codebox}
\end{algorithm}


\end{document}
